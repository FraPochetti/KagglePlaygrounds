{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DeepFake data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Handling raw data from Kaggle"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "df = pd.read_json(\"../data/dfdc_train_part_5/metadata.json\", \n",
    "                    orient=\"index\").reset_index().rename(columns={'index': 'video'})\n",
    "df.label.value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "FAKE    2123\n",
       "REAL     360\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Creating a balanced dataset with all `REAL` videos (360) and an equal number of randomly selected `FAKE`s.\n",
    "2. Splitting into training (80%) and validation (20%). "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "data_df = df.loc[df.label==\"REAL\"].append(df.loc[df.label==\"FAKE\"].sample(360, random_state=0))\n",
    "data_df = data_df.sample(len(data_df), random_state=0)\n",
    "data_df[\"split\"] = int(len(data_df)*0.2) * [\"valid\"] + int(len(data_df)*0.8) * [\"train\"]\n",
    "data_df[\"target\"] = np.where(data_df.label==\"FAKE\", 1, 0)\n",
    "data_df[\"filepath\"] = data_df.video.apply(lambda x: \"/home/ubuntu/data/dfdc_train_part_5/\"+x)\n",
    "data_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               video label  split        original  target  \\\n",
       "1883  aonaupzilc.mp4  FAKE  valid  btsbqigrcq.mp4       1   \n",
       "1501  wbhxgtaswg.mp4  FAKE  valid  ubarhvreoj.mp4       1   \n",
       "2161  dfjueqzmfx.mp4  REAL  valid             NaN       0   \n",
       "1901  tgwvzoncvd.mp4  REAL  valid             NaN       0   \n",
       "1705  knbsiylwur.mp4  FAKE  valid  ebuebnxpdq.mp4       1   \n",
       "\n",
       "                                               filepath  \n",
       "1883  /home/ubuntu/data/dfdc_train_part_5/aonaupzilc...  \n",
       "1501  /home/ubuntu/data/dfdc_train_part_5/wbhxgtaswg...  \n",
       "2161  /home/ubuntu/data/dfdc_train_part_5/dfjueqzmfx...  \n",
       "1901  /home/ubuntu/data/dfdc_train_part_5/tgwvzoncvd...  \n",
       "1705  /home/ubuntu/data/dfdc_train_part_5/knbsiylwur...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "      <th>original</th>\n",
       "      <th>target</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1883</th>\n",
       "      <td>aonaupzilc.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>valid</td>\n",
       "      <td>btsbqigrcq.mp4</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/ubuntu/data/dfdc_train_part_5/aonaupzilc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>wbhxgtaswg.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>valid</td>\n",
       "      <td>ubarhvreoj.mp4</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/ubuntu/data/dfdc_train_part_5/wbhxgtaswg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2161</th>\n",
       "      <td>dfjueqzmfx.mp4</td>\n",
       "      <td>REAL</td>\n",
       "      <td>valid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/ubuntu/data/dfdc_train_part_5/dfjueqzmfx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901</th>\n",
       "      <td>tgwvzoncvd.mp4</td>\n",
       "      <td>REAL</td>\n",
       "      <td>valid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/ubuntu/data/dfdc_train_part_5/tgwvzoncvd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1705</th>\n",
       "      <td>knbsiylwur.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>valid</td>\n",
       "      <td>ebuebnxpdq.mp4</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/ubuntu/data/dfdc_train_part_5/knbsiylwur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "data_df.groupby([\"split\", \"label\"]).video.count()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "split  label\n",
       "train  FAKE     284\n",
       "       REAL     292\n",
       "valid  FAKE      76\n",
       "       REAL      68\n",
       "Name: video, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Saving filepath and binary target in TSV format as expected by PyTorchVideo (this is just one of the many ways to feed data into a PyTorchVideo model)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_df.loc[data_df.split==\"valid\", [\"filepath\", \"target\"]].to_csv(\"valid.csv\",\n",
    "                                                                   sep=\"\\t\", index=False, header=None)\n",
    "data_df.loc[data_df.split==\"train\", [\"filepath\", \"target\"]].to_csv(\"train.csv\", \n",
    "                                                                   sep=\"\\t\", index=False, header=None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "! tail train.csv"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/ubuntu/data/dfdc_train_part_5/hmnspetjoc.mp4\t0\n",
      "/home/ubuntu/data/dfdc_train_part_5/yynszivwba.mp4\t1\n",
      "/home/ubuntu/data/dfdc_train_part_5/csomiogcwu.mp4\t0\n",
      "/home/ubuntu/data/dfdc_train_part_5/lfiyoyymnn.mp4\t0\n",
      "/home/ubuntu/data/dfdc_train_part_5/tohrqjyter.mp4\t0\n",
      "/home/ubuntu/data/dfdc_train_part_5/hblkonvlyq.mp4\t1\n",
      "/home/ubuntu/data/dfdc_train_part_5/xsgmwwbeib.mp4\t0\n",
      "/home/ubuntu/data/dfdc_train_part_5/emsnbjdsve.mp4\t1\n",
      "/home/ubuntu/data/dfdc_train_part_5/bxtaeanoyr.mp4\t1\n",
      "/home/ubuntu/data/dfdc_train_part_5/bpzmcgorle.mp4\t1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Face cropping\n",
    "\n",
    "Taking a look at the data it seemed apparent that, for deepfake videos, the manipulation had occurred in the facial region of the displayed individual.\n",
    "\n",
    "Given the original MP4s are `1928 x 1080` pixels, and the face occupies only a small region of the frame, we are feeding the model with a lot of useless information. \n",
    "\n",
    "On top of it, as we have to resize the original frames anyway as they are too big to fit on GPU, we are losing useful signal.\n",
    "\n",
    "So, I shamelessly copied the approach showcased in [this](https://www.kaggle.com/timesler/fast-mtcnn-detector-55-fps-at-full-resolution) Kaggle kernel.\n",
    "\n",
    "The idea is to use MTCNN (shipped with the [`facenet_pytorch`](https://github.com/timesler/facenet-pytorch) package) to detect faces every N frames of each video and stack them together into a new MP4 file.\n",
    "\n",
    "We will get away lighter result, 10 vs 30 original FPS and `256 x 256` vs `1928 x 1080` size, focused on our region of interest."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from facenet_pytorch import MTCNN\n",
    "from PIL import Image\n",
    "import torch\n",
    "from imutils.video import FileVideoStream\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import ffmpeg\n",
    "import pandas as pd\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "tr = pd.read_csv(\"train.csv\", sep=\"\\t\", names=[\"fname\", \"label\"])\n",
    "va = pd.read_csv(\"valid.csv\", sep=\"\\t\", names=[\"fname\", \"label\"])\n",
    "df = tr.append(va)\n",
    "filenames = df.fname.tolist()\n",
    "len(filenames)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "720"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def to_image(array): return Image.fromarray(np.moveaxis(array.squeeze().numpy().astype(np.uint8),0, -1))\n",
    "\n",
    "class FastMTCNN(object):\n",
    "    \"\"\"Fast MTCNN implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, stride, resize=1, *args, **kwargs):\n",
    "        \"\"\"Constructor for FastMTCNN class.\n",
    "        \n",
    "        Arguments:\n",
    "            stride (int): The detection stride. Faces will be detected every `stride` frames\n",
    "                and remembered for `stride-1` frames.\n",
    "        \n",
    "        Keyword arguments:\n",
    "            resize (float): Fractional frame scaling. [default: {1}]\n",
    "            *args: Arguments to pass to the MTCNN constructor. See help(MTCNN).\n",
    "            **kwargs: Keyword arguments to pass to the MTCNN constructor. See help(MTCNN).\n",
    "        \"\"\"\n",
    "        self.stride = stride\n",
    "        self.resize = resize\n",
    "        self.mtcnn = MTCNN(*args, **kwargs)\n",
    "        \n",
    "    def __call__(self, frames):\n",
    "        \"\"\"Detect faces in frames using strided MTCNN.\"\"\"\n",
    "        if self.resize != 1:\n",
    "            frames = [\n",
    "                cv2.resize(f, (int(f.shape[1] * self.resize), int(f.shape[0] * self.resize)))\n",
    "                    for f in frames\n",
    "            ]\n",
    "                      \n",
    "        faces = self.mtcnn.forward(frames[::self.stride])\n",
    "        return faces\n",
    "\n",
    "class FacesVideo:\n",
    "    def __init__(self, filenames, fast_mtcnn, batch_size=60, framerate=10):\n",
    "        self.filenames = filenames\n",
    "        self.fast_mtcnn = fast_mtcnn\n",
    "        self.batch_size = batch_size\n",
    "        self.framerate = framerate\n",
    "\n",
    "    def extract_faces(self, filename):\n",
    "        frames = []\n",
    "\n",
    "        v_cap = FileVideoStream(filename).start()\n",
    "        v_len = int(v_cap.stream.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        faces = []\n",
    "\n",
    "        for j in range(v_len):\n",
    "\n",
    "            frame = v_cap.read()\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) >= self.batch_size or j == v_len - 1:\n",
    "                faces += self.fast_mtcnn(frames)\n",
    "                frames = []\n",
    "\n",
    "        v_cap.stop()\n",
    "        faces = self.check_faces(faces)\n",
    "\n",
    "        return faces\n",
    "\n",
    "    def get_first_non_none(self, faces):\n",
    "        for face in faces:\n",
    "            if face is not None:\n",
    "                return face\n",
    "\n",
    "    def check_faces(self, faces):\n",
    "        for i, face in enumerate(faces):\n",
    "            if face is None:\n",
    "                if i > (len(faces)-10):\n",
    "                    faces[i] = self.get_first_non_none(faces[(i-10):])\n",
    "                else:\n",
    "                    faces[i] = self.get_first_non_none(faces[i:])\n",
    "        return faces\n",
    "\n",
    "    def save_frames(self, faces, filename):\n",
    "        new_path = Path(filename.replace(\".mp4\", \"\").replace(\"dfdc_train_part_5\", \"faces/frames\"))\n",
    "        new_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for i, face in enumerate(faces):\n",
    "            to_image(face).save(new_path / f\"{str(i).zfill(3)}.jpg\")\n",
    "        return new_path\n",
    "\n",
    "    def collate_video(self, path): \n",
    "        movie_path = str(path).replace(\"frames\", \"videos\") + \"_face.mp4\"\n",
    "        ffmpeg.input(str(path) + '/*.jpg', pattern_type='glob', framerate=self.framerate).output(movie_path).overwrite_output().run()\n",
    "\n",
    "    def get_face_videos(self, filename):\n",
    "        faces = self.extract_faces(filename)\n",
    "        faces_path = self.save_frames(faces, filename)\n",
    "        self.collate_video(faces_path) \n",
    "        \n",
    "        \n",
    "    def run(self): \n",
    "        for f in tqdm(self.filenames):\n",
    "            self.get_face_videos(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fast_mtcnn = FastMTCNN(\n",
    "    stride=3,\n",
    "    resize=1,\n",
    "    margin=30,\n",
    "    factor=0.6,\n",
    "    keep_all=False,\n",
    "    device=device,\n",
    "    post_process=False,\n",
    ")\n",
    "\n",
    "# here we use a `framerate` of 10 FPS. \n",
    "# 3 times less compares to the \n",
    "fv = FacesVideo(filenames, fast_mtcnn)\n",
    "fv.run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Take a sneak peek into videos.\n",
    "\n",
    "### Face cropped video..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "# looking into FAKE (resampled) videos (`label==1`)\n",
    "# fn = df.loc[df.label==1].fname.sample().values[0]\n",
    "path = \"/home/ubuntu/data/dfdc_train_part_5/iwladlmomt.mp4\"\n",
    "fn = path.replace(\".mp4\", \"_face.mp4\").replace(\"dfdc_train_part_5\", \"faces/videos\")\n",
    "Video(fn)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<IPython.core.display.Video object>"
      ],
      "text/html": [
       "<video src=\"/home/ubuntu/data/faces/videos/iwladlmomt_face.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ... and the original MP4 file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "Video(path)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<IPython.core.display.Video object>"
      ],
      "text/html": [
       "<video src=\"/home/ubuntu/data/dfdc_train_part_5/iwladlmomt.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Saving new TSV files for PyTorchVideo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "va[\"face_fname\"] = va.fname.apply(lambda x: x.replace(\".mp4\", \"_face.mp4\").replace(\"dfdc_train_part_5\", \"faces/videos\"))\n",
    "tr[\"face_fname\"] = tr.fname.apply(lambda x: x.replace(\".mp4\", \"_face.mp4\").replace(\"dfdc_train_part_5\", \"faces/videos\"))\n",
    "\n",
    "va[[\"face_fname\", \"label\"]].to_csv(\"valid_faces.csv\", sep=\"\\t\", index=False, header=None)\n",
    "tr[[\"face_fname\", \"label\"]].to_csv(\"train_faces.csv\", sep=\"\\t\", index=False, header=None)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit"
  },
  "interpreter": {
   "hash": "e569a953c5897a02fa9ce14fbb030fd88aea015fed6417ed7c79256ee8cebec4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}