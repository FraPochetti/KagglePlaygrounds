{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addressing Online Hate Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 8, 6\n",
    "\n",
    "import spacy\n",
    "import re, string\n",
    "import boto3\n",
    "#import gensim\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity='all'\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "\n",
    "import mxnet as mx\n",
    "import gluonnlp as nlp\n",
    "from mxnet import nd, gluon, autograd\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "mx.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "class NbSvmClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0, dual=False, n_jobs=1):\n",
    "        self.C = C\n",
    "        self.dual = dual\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict(x.multiply(self._r))\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict_proba(x.multiply(self._r))\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # Check that X and y have correct shape\n",
    "        y = y.values\n",
    "        x, y = check_X_y(x, y, accept_sparse=True)\n",
    "\n",
    "        def pr(x, y_i, y):\n",
    "            p = x[y==y_i].sum(0)\n",
    "            return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "        self._r = sparse.csr_matrix(np.log(pr(x,1,y) / pr(x,0,y)))\n",
    "        x_nb = x.multiply(self._r)\n",
    "        self._clf = LogisticRegression(C=self.C, dual=self.dual, n_jobs=self.n_jobs).fit(x_nb, y)\n",
    "        return self\n",
    "    \n",
    "##############################################################################\n",
    "\n",
    "def add_glove(X, dim=300): return np.mean([w2v[word] if word in w2v else np.zeros(dim) for word in tokenize(X)], axis=0)\n",
    "\n",
    "def add_gensim(X, dim=300): return np.mean([gens.wv[word] if word in gens.wv else np.zeros(dim) for word in tokenize(X)], axis=0)\n",
    "\n",
    "##############################################################################\n",
    "## USED IN THE DEEP LEARNING CHUNK\n",
    "##############################################################################\n",
    "\n",
    "def evaluate(net, dataloader, context):\n",
    "    total_L = 0.0\n",
    "    total_sample_num = 0\n",
    "    total_correct_num = 0\n",
    "    start_log_interval_time = time.time()\n",
    "    print('Begin Testing...')\n",
    "    for i, ((data, valid_length), label) in enumerate(dataloader):\n",
    "        data = mx.nd.transpose(data.as_in_context(context))\n",
    "        valid_length = valid_length.as_in_context(context).astype(np.float32)\n",
    "        label = label.as_in_context(context)\n",
    "        output = net(data, valid_length)\n",
    "        L = loss(output, label)\n",
    "        pred = (output > 0.5)\n",
    "        total_L += L.sum().asscalar()\n",
    "        total_sample_num += (label.shape[0] * label.shape[1])\n",
    "        total_correct_num += (pred == label).sum().asscalar()\n",
    "        \n",
    "        if i == 0:\n",
    "            labels = label\n",
    "            outputs = output\n",
    "        else:\n",
    "            labels = mx.ndarray.concatenate([labels, label])\n",
    "            outputs = mx.ndarray.concatenate([outputs, output])\n",
    "        \n",
    "        if (i + 1) % log_interval == 0:\n",
    "            print('[Batch {}/{}] elapsed {:.2f} s'.format(\n",
    "                i + 1, len(dataloader),\n",
    "                time.time() - start_log_interval_time))\n",
    "            start_log_interval_time = time.time()\n",
    "    avg_L = total_L / float(total_sample_num)\n",
    "    acc = total_correct_num / float(total_sample_num)\n",
    "    auc = roc_auc_score(labels.as_in_context(mx.cpu()).asnumpy(), \n",
    "                        outputs.as_in_context(mx.cpu()).asnumpy(), average='macro')\n",
    "    return avg_L, acc, auc\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "def get_dataloader():\n",
    "    # Construct the DataLoader\n",
    "    # Pad data, stack label and lengths\n",
    "    batchify_fn = nlp.data.batchify.Tuple(\n",
    "        nlp.data.batchify.Pad(axis=0, ret_length=True),\n",
    "        nlp.data.batchify.Stack(dtype='float32'))\n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(\n",
    "        train_data_lengths,\n",
    "        batch_size=batch_size,\n",
    "        num_buckets=bucket_num,\n",
    "        ratio=bucket_ratio,\n",
    "        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "    train_dataloader = gluon.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        batchify_fn=batchify_fn)\n",
    "    test_dataloader = gluon.data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        batchify_fn=batchify_fn)\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "def train(net, context, epochs):\n",
    "    trainer = gluon.Trainer(net.collect_params(), optim,\n",
    "                            {'learning_rate': learning_rate})\n",
    "\n",
    "    parameters = net.collect_params().values()\n",
    "\n",
    "    # Training/Testing\n",
    "    for epoch in range(epochs):\n",
    "        # Epoch training stats\n",
    "        start_epoch_time = time.time()\n",
    "        epoch_L = 0.0\n",
    "        epoch_sent_num = 0\n",
    "        epoch_wc = 0\n",
    "        # Log interval training stats\n",
    "        start_log_interval_time = time.time()\n",
    "        log_interval_wc = 0\n",
    "        log_interval_sent_num = 0\n",
    "        log_interval_L = 0.0\n",
    "\n",
    "        for i, ((data, length), label) in enumerate(train_dataloader):\n",
    "            L = 0\n",
    "            wc = length.sum().asscalar()\n",
    "            log_interval_wc += wc\n",
    "            epoch_wc += wc\n",
    "            log_interval_sent_num += data.shape[1]\n",
    "            epoch_sent_num += data.shape[1]\n",
    "            with autograd.record():\n",
    "                output = net(data.as_in_context(context).T,\n",
    "                             length.as_in_context(context).astype(np.float32))\n",
    "                L = L + loss(output, label.as_in_context(context)).sum()\n",
    "            L.backward()\n",
    "            # Clip gradient\n",
    "            if grad_clip:\n",
    "                gluon.utils.clip_global_norm(\n",
    "                    [p.grad(context) for p in parameters],\n",
    "                    grad_clip)\n",
    "            # Update parameter\n",
    "            trainer.step(1)\n",
    "            log_interval_L += L.asscalar()\n",
    "            epoch_L += L.asscalar()\n",
    "            if (i + 1) % log_interval == 0:\n",
    "                print(\n",
    "                    '[Epoch {} Batch {}/{}] elapsed {:.2f} s, '\n",
    "                    'avg loss {:.6f}, throughput {:.2f}K wps'.format(\n",
    "                        epoch, i + 1, len(train_dataloader),\n",
    "                        time.time() - start_log_interval_time,\n",
    "                        log_interval_L / log_interval_sent_num, log_interval_wc\n",
    "                        / 1000 / (time.time() - start_log_interval_time)))\n",
    "                # Clear log interval training stats\n",
    "                start_log_interval_time = time.time()\n",
    "                log_interval_wc = 0\n",
    "                log_interval_sent_num = 0\n",
    "                log_interval_L = 0\n",
    "        end_epoch_time = time.time()\n",
    "        test_avg_L, test_acc, test_auc = evaluate(net, test_dataloader, context)\n",
    "        print('[Epoch {}] train avg loss {:.6f}, test acc {:.5f}, '\n",
    "              'test avg loss {:.6f}, test auc {:.5f}'.format(\n",
    "                  epoch, epoch_L / epoch_sent_num, test_acc, test_avg_L,\n",
    "                  test_auc))\n",
    "        \n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"eider-pochetti\"\n",
    "file_name = \"train.csv\"\n",
    "\n",
    "s3 = boto3.client('s3') \n",
    "obj = s3.get_object(Bucket= bucket, Key= file_name) \n",
    "df = pd.read_csv(obj['Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('./data/toxic/train.csv')#'C:\\\\Users\\\\pochetti\\\\WorkDocs\\\\Desktop\\\\Fra\\\\Francesco\\\\Kaggle\\\\toxic\\\\train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick EDA and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  other  \n",
       "0             0        0       0       0              0      1  \n",
       "1             0        0       0       0              0      1  \n",
       "2             0        0       0       0              0      1  \n",
       "3             0        0       0       0              0      1  \n",
       "4             0        0       0       0              0      1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_dummies = ['id', 'comment_text']\n",
    "other = df.loc[:,[col for col in df.columns if col not in not_dummies]].sum(axis=1)\n",
    "df['other'] = np.where(other == 0, 1, 0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic             15294\n",
       "severe_toxic       1595\n",
       "obscene            8449\n",
       "threat              478\n",
       "insult             7877\n",
       "identity_hate      1405\n",
       "other            143346\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:,[col for col in df.columns if col not in not_dummies]].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row0_col0 {\n",
       "            background-color:  #023858;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row0_col1 {\n",
       "            background-color:  #81aed2;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row0_col2 {\n",
       "            background-color:  #05659f;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row0_col3 {\n",
       "            background-color:  #c8cde4;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row0_col4 {\n",
       "            background-color:  #0569a4;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row0_col5 {\n",
       "            background-color:  #91b5d6;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row0_col6 {\n",
       "            background-color:  #fff7fb;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row1_col0 {\n",
       "            background-color:  #2c89bd;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row1_col1 {\n",
       "            background-color:  #023858;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row1_col2 {\n",
       "            background-color:  #2c89bd;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row1_col3 {\n",
       "            background-color:  #d1d2e6;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row1_col4 {\n",
       "            background-color:  #358fc0;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row1_col5 {\n",
       "            background-color:  #a5bddb;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row1_col6 {\n",
       "            background-color:  #b1c2de;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row2_col0 {\n",
       "            background-color:  #046198;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row2_col1 {\n",
       "            background-color:  #60a1ca;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row2_col2 {\n",
       "            background-color:  #023858;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row2_col3 {\n",
       "            background-color:  #cdd0e5;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row2_col4 {\n",
       "            background-color:  #045f95;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row2_col5 {\n",
       "            background-color:  #8bb2d4;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row2_col6 {\n",
       "            background-color:  #eae6f1;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row3_col0 {\n",
       "            background-color:  #509ac6;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row3_col1 {\n",
       "            background-color:  #b7c5df;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row3_col2 {\n",
       "            background-color:  #76aad0;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row3_col3 {\n",
       "            background-color:  #023858;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row3_col4 {\n",
       "            background-color:  #76aad0;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row3_col5 {\n",
       "            background-color:  #bcc7e1;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row3_col6 {\n",
       "            background-color:  #99b8d8;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row4_col0 {\n",
       "            background-color:  #04639b;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row4_col1 {\n",
       "            background-color:  #6ba5cd;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row4_col2 {\n",
       "            background-color:  #045e94;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row4_col3 {\n",
       "            background-color:  #cacee5;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row4_col4 {\n",
       "            background-color:  #023858;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row4_col5 {\n",
       "            background-color:  #7bacd1;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row4_col6 {\n",
       "            background-color:  #e7e3f0;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row5_col0 {\n",
       "            background-color:  #358fc0;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row5_col1 {\n",
       "            background-color:  #a2bcda;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row5_col2 {\n",
       "            background-color:  #4c99c5;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row5_col3 {\n",
       "            background-color:  #d2d3e7;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row5_col4 {\n",
       "            background-color:  #4094c3;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row5_col5 {\n",
       "            background-color:  #023858;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row5_col6 {\n",
       "            background-color:  #afc1dd;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row6_col0 {\n",
       "            background-color:  #fff7fb;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row6_col1 {\n",
       "            background-color:  #fff7fb;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row6_col2 {\n",
       "            background-color:  #fff7fb;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row6_col3 {\n",
       "            background-color:  #fff7fb;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row6_col4 {\n",
       "            background-color:  #fff7fb;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row6_col5 {\n",
       "            background-color:  #fff7fb;\n",
       "        }    #T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row6_col6 {\n",
       "            background-color:  #023858;\n",
       "        }</style>  \n",
       "<table id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2\" > \n",
       "<thead>    <tr> \n",
       "        <th class=\"blank level0\" ></th> \n",
       "        <th class=\"col_heading level0 col0\" >toxic</th> \n",
       "        <th class=\"col_heading level0 col1\" >severe_toxic</th> \n",
       "        <th class=\"col_heading level0 col2\" >obscene</th> \n",
       "        <th class=\"col_heading level0 col3\" >threat</th> \n",
       "        <th class=\"col_heading level0 col4\" >insult</th> \n",
       "        <th class=\"col_heading level0 col5\" >identity_hate</th> \n",
       "        <th class=\"col_heading level0 col6\" >other</th> \n",
       "    </tr></thead> \n",
       "<tbody>    <tr> \n",
       "        <th id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2level0_row0\" class=\"row_heading level0 row0\" >toxic</th> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row0_col0\" class=\"data row0 col0\" >1</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row0_col1\" class=\"data row0 col1\" >0.308619</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row0_col2\" class=\"data row0 col2\" >0.676515</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row0_col3\" class=\"data row0 col3\" >0.157058</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row0_col4\" class=\"data row0 col4\" >0.647518</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row0_col5\" class=\"data row0 col5\" >0.266009</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row0_col6\" class=\"data row0 col6\" >-0.967748</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2level0_row1\" class=\"row_heading level0 row1\" >severe_toxic</th> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row1_col0\" class=\"data row1 col0\" >0.308619</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row1_col1\" class=\"data row1 col1\" >1</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row1_col2\" class=\"data row1 col2\" >0.403014</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row1_col3\" class=\"data row1 col3\" >0.123601</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row1_col4\" class=\"data row1 col4\" >0.375807</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row1_col5\" class=\"data row1 col5\" >0.2016</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row1_col6\" class=\"data row1 col6\" >-0.298666</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2level0_row2\" class=\"row_heading level0 row2\" >obscene</th> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row2_col0\" class=\"data row2 col0\" >0.676515</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row2_col1\" class=\"data row2 col1\" >0.403014</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row2_col2\" class=\"data row2 col2\" >1</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row2_col3\" class=\"data row2 col3\" >0.141179</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row2_col4\" class=\"data row2 col4\" >0.741272</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row2_col5\" class=\"data row2 col5\" >0.286867</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row2_col6\" class=\"data row2 col6\" >-0.702812</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2level0_row3\" class=\"row_heading level0 row3\" >threat</th> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row3_col0\" class=\"data row3 col0\" >0.157058</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row3_col1\" class=\"data row3 col1\" >0.123601</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row3_col2\" class=\"data row3 col2\" >0.141179</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row3_col3\" class=\"data row3 col3\" >1</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row3_col4\" class=\"data row3 col4\" >0.150022</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row3_col5\" class=\"data row3 col5\" >0.115128</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row3_col6\" class=\"data row3 col6\" >-0.162925</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2level0_row4\" class=\"row_heading level0 row4\" >insult</th> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row4_col0\" class=\"data row4 col0\" >0.647518</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row4_col1\" class=\"data row4 col1\" >0.375807</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row4_col2\" class=\"data row4 col2\" >0.741272</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row4_col3\" class=\"data row4 col3\" >0.150022</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row4_col4\" class=\"data row4 col4\" >1</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row4_col5\" class=\"data row4 col5\" >0.337736</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row4_col6\" class=\"data row4 col6\" >-0.677324</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2level0_row5\" class=\"row_heading level0 row5\" >identity_hate</th> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row5_col0\" class=\"data row5 col0\" >0.266009</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row5_col1\" class=\"data row5 col1\" >0.2016</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row5_col2\" class=\"data row5 col2\" >0.286867</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row5_col3\" class=\"data row5 col3\" >0.115128</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row5_col4\" class=\"data row5 col4\" >0.337736</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row5_col5\" class=\"data row5 col5\" >1</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row5_col6\" class=\"data row5 col6\" >-0.280144</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2level0_row6\" class=\"row_heading level0 row6\" >other</th> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row6_col0\" class=\"data row6 col0\" >-0.967748</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row6_col1\" class=\"data row6 col1\" >-0.298666</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row6_col2\" class=\"data row6 col2\" >-0.702812</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row6_col3\" class=\"data row6 col3\" >-0.162925</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row6_col4\" class=\"data row6 col4\" >-0.677324</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row6_col5\" class=\"data row6 col5\" >-0.280144</td> \n",
       "        <td id=\"T_e8cbfcd0_b44d_11e8_a1a7_fd4ab57ef7f2row6_col6\" class=\"data row6 col6\" >1</td> \n",
       "    </tr></tbody> \n",
       "</table> "
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fa318079a90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = df.loc[:,[col for col in df.columns if col not in not_dummies]].corr()\n",
    "corr.style.background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENT = 'comment_text'\n",
    "df[COMMENT].fillna(\"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119678,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(119678, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(39893,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(39893, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[COMMENT], \n",
    "                                                    df.loc[:,[col for col in df.columns if col not in not_dummies]], \n",
    "                                                    test_size=0.25, random_state=4)\n",
    "\n",
    "X_train.shape\n",
    "y_train.shape\n",
    "\n",
    "X_test.shape\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive-Bayes-SVM on top of Tf-IDF (ROC-AUC: 0.98251)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X_train.shape[0]\n",
    "\n",
    "vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1 )\n",
    "\n",
    "trn_term_doc = vec.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_term_doc = vec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<119678x339500 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 13193559 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit toxic\n",
      "fit severe_toxic\n",
      "fit obscene\n",
      "fit threat\n",
      "fit insult\n",
      "fit identity_hate\n"
     ]
    }
   ],
   "source": [
    "preds = np.zeros((len(X_test), len(label_cols)))\n",
    "\n",
    "for i, j in enumerate(label_cols):\n",
    "    print('fit', j)\n",
    "    m = NbSvmClassifier(C=4, dual=True, n_jobs=-1).fit(trn_term_doc, y_train[j])\n",
    "    preds[:,i] = m.predict_proba(test_term_doc)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98251143449724354"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test[label_cols].values, preds, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression on top of Tf-IDF (ROC-AUC: 0.97941)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          n_jobs=-1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = OneVsRestClassifier(LogisticRegression(), n_jobs=-1)\n",
    "clf.fit(trn_term_doc, y_train[label_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97941168593628047"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test[label_cols].values, clf.predict_proba(test_term_doc), average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression on top of pre-trained Glove Word Embeddings (ROC-AUC: 0.94220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.read_csv('C:\\\\Users\\\\pochetti\\\\WorkDocs\\\\Desktop\\\\Fra\\\\Francesco\\\\Copperfield\\\\glove.6B\\\\glove.6B.300d.txt', sep=' ', \n",
    "                    header=None, quoting = 3)\n",
    "words = words.apply(pd.to_numeric, errors='ignore')\n",
    "d = {'word': words.loc[:,0].tolist(), 'embedding': words.loc[:,1:].values.tolist()}\n",
    "d50 = pd.DataFrame(data=d)\n",
    "w2v = d50.set_index('word')['embedding'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v['book'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = X_train.apply(add_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "tra = np.array(tr.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          n_jobs=-1)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glo = OneVsRestClassifier(LogisticRegression(), n_jobs=-1)\n",
    "glo.fit(tra, y_train[label_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tst = X_test.apply(add_glove)\n",
    "tsta = np.array(tst.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.942208144506225"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test[label_cols].values, glo.predict_proba(tsta), average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression on top of ad-hoc trained Gensim Embeddings (ROC-AUC: 0.95545)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = df.comment_text.apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = l.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97376954, 136103550)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gens = gensim.models.Word2Vec(documents, size=300, window=10, min_count=2, workers=10)\n",
    "\n",
    "gens.train(documents, total_examples=len(documents), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gens.wv['computer'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gens.save(\"word2vecgensim.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_gensim = X_train.apply(add_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tra_gensim = np.array(tr_gensim.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          n_jobs=-1)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = OneVsRestClassifier(LogisticRegression(), n_jobs=-1)\n",
    "g.fit(tra_gensim, y_train[label_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tst_gensim = X_test.apply(add_gensim)\n",
    "tsta_gensim = np.array(tst_gensim.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9554535195331965"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test[label_cols].values, g.predict_proba(tsta_gensim), average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning with GluonNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPoolingLayer(gluon.HybridBlock):\n",
    "    \"\"\"A block for mean pooling of encoder features\"\"\"\n",
    "    def __init__(self, prefix=None, params=None):\n",
    "        super(MeanPoolingLayer, self).__init__(prefix=prefix, params=params)\n",
    "\n",
    "    def hybrid_forward(self, F, data, valid_length): # pylint: disable=arguments-differ\n",
    "        \"\"\"Forward logic\"\"\"\n",
    "        # Data will have shape (T, N, C)\n",
    "        masked_encoded = F.SequenceMask(data,\n",
    "                                        sequence_length=valid_length,\n",
    "                                        use_sequence_length=True)\n",
    "        agg_state = F.broadcast_div(F.sum(masked_encoded, axis=0),\n",
    "                                    F.expand_dims(valid_length, axis=1))\n",
    "        return agg_state\n",
    "\n",
    "\n",
    "class SentimentNet(gluon.HybridBlock):\n",
    "    \"\"\"Network for sentiment analysis.\"\"\"\n",
    "    def __init__(self, dropout, prefix=None, params=None):\n",
    "        super(SentimentNet, self).__init__(prefix=prefix, params=params)\n",
    "        with self.name_scope():\n",
    "            self.embedding = None # will set with lm embedding later\n",
    "            self.encoder = None # will set with lm encoder later\n",
    "            self.agg_layer = MeanPoolingLayer()\n",
    "            self.output = gluon.nn.HybridSequential()\n",
    "            with self.output.name_scope():\n",
    "                #self.output.add(gluon.nn.BatchNorm(axis=1, center=True, scale=True))\n",
    "                self.output.add(gluon.nn.Dropout(dropout))\n",
    "                self.output.add(gluon.nn.Dense(len(label_cols), flatten=False, activation='sigmoid'))\n",
    "\n",
    "    def hybrid_forward(self, F, data, valid_length): # pylint: disable=arguments-differ\n",
    "        encoded = self.encoder(self.embedding(data))  # Shape(T, N, C)\n",
    "        agg_state = self.agg_layer(encoded, valid_length)\n",
    "        out = self.output(agg_state)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.5\n",
    "language_model_name = 'standard_lstm_lm_200'\n",
    "pretrained = True\n",
    "learning_rate, batch_size = 0.005, 16\n",
    "bucket_num, bucket_ratio = 10, 0.2\n",
    "epochs = 3\n",
    "grad_clip = None\n",
    "log_interval = 1000\n",
    "context = mx.gpu(0)\n",
    "loss = gluon.loss.SigmoidBCELoss(from_sigmoid=True)\n",
    "optim = 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_model, vocab = nlp.model.get_model(name=language_model_name,\n",
    "                                      dataset_name='wikitext-2',\n",
    "                                      pretrained=pretrained,\n",
    "                                      ctx=context,\n",
    "                                      dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentNet(\n",
      "  (embedding): HybridSequential(\n",
      "    (0): Embedding(33278 -> 200, float32)\n",
      "    (1): Dropout(p = 0.5, axes=())\n",
      "  )\n",
      "  (encoder): LSTM(200 -> 200, TNC, num_layers=2, dropout=0.5)\n",
      "  (agg_layer): MeanPoolingLayer(\n",
      "  \n",
      "  )\n",
      "  (output): HybridSequential(\n",
      "    (0): Dropout(p = 0.5, axes=())\n",
      "    (1): Dense(None -> 6, Activation(sigmoid))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = SentimentNet(dropout=dropout)\n",
    "net.embedding = lm_model.embedding\n",
    "net.encoder = lm_model.encoder\n",
    "net.hybridize()\n",
    "net.output.initialize(mx.init.Xavier(), ctx=context)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nlp.data.SpacyTokenizer('en')\n",
    "length_clip = nlp.data.ClipSequence(500)\n",
    "\n",
    "def my_tokens(s): return vocab[length_clip(tokenize(s))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tok = X_train.apply(my_tokens)\n",
    "X_test_tok = X_test.apply(my_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = mx.gluon.data.ArrayDataset(X_train_tok.values, y_train[label_cols].values)\n",
    "train_data_lengths = X_train_tok.str.len().values\n",
    "\n",
    "test_dataset = mx.gluon.data.ArrayDataset(X_test_tok.values, y_test[label_cols].values)\n",
    "test_data_lengths = X_test_tok.str.len().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=119678, batch_num=5668\n",
      "  key=[59, 108, 157, 206, 255, 304, 353, 402, 451, 500]\n",
      "  cnt=[71327, 22968, 9781, 5105, 3296, 1834, 1273, 727, 583, 2784]\n",
      "  batch_size=[27, 16, 16, 16, 16, 16, 16, 16, 16, 16]\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, test_dataloader = get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 1000/5668] elapsed 67.39 s, avg loss 0.019530, throughput 25.71K wps\n",
      "[Epoch 0 Batch 2000/5668] elapsed 66.42 s, avg loss 0.015655, throughput 26.50K wps\n",
      "[Epoch 0 Batch 3000/5668] elapsed 65.71 s, avg loss 0.014912, throughput 26.05K wps\n",
      "[Epoch 0 Batch 4000/5668] elapsed 64.37 s, avg loss 0.014517, throughput 25.24K wps\n",
      "[Epoch 0 Batch 5000/5668] elapsed 64.94 s, avg loss 0.014598, throughput 25.71K wps\n",
      "Begin Testing...\n",
      "[Batch 1000/2494] elapsed 70.14 s\n",
      "[Batch 2000/2494] elapsed 70.17 s\n",
      "[Epoch 0] train avg loss 0.015724, test acc 0.97182, test avg loss 0.013366, test auc 0.95026\n",
      "[Epoch 1 Batch 1000/5668] elapsed 65.81 s, avg loss 0.014073, throughput 24.81K wps\n",
      "[Epoch 1 Batch 2000/5668] elapsed 66.53 s, avg loss 0.012863, throughput 26.44K wps\n",
      "[Epoch 1 Batch 3000/5668] elapsed 65.43 s, avg loss 0.013637, throughput 25.96K wps\n",
      "[Epoch 1 Batch 4000/5668] elapsed 65.38 s, avg loss 0.014046, throughput 25.90K wps\n",
      "[Epoch 1 Batch 5000/5668] elapsed 64.90 s, avg loss 0.013654, throughput 25.50K wps\n",
      "Begin Testing...\n",
      "[Batch 1000/2494] elapsed 70.24 s\n",
      "[Batch 2000/2494] elapsed 70.03 s\n",
      "[Epoch 1] train avg loss 0.013671, test acc 0.97398, test avg loss 0.012309, test auc 0.95160\n",
      "[Epoch 2 Batch 1000/5668] elapsed 67.96 s, avg loss 0.012940, throughput 26.03K wps\n",
      "[Epoch 2 Batch 2000/5668] elapsed 64.83 s, avg loss 0.014872, throughput 25.61K wps\n",
      "[Epoch 2 Batch 3000/5668] elapsed 65.44 s, avg loss 0.016309, throughput 25.92K wps\n",
      "[Epoch 2 Batch 4000/5668] elapsed 65.70 s, avg loss 0.015908, throughput 26.09K wps\n",
      "[Epoch 2 Batch 5000/5668] elapsed 65.59 s, avg loss 0.015455, throughput 25.87K wps\n",
      "Begin Testing...\n",
      "[Batch 1000/2494] elapsed 69.87 s\n",
      "[Batch 2000/2494] elapsed 70.06 s\n",
      "[Epoch 2] train avg loss 0.015334, test acc 0.97243, test avg loss 0.013464, test auc 0.93872\n",
      "[Epoch 3 Batch 1000/5668] elapsed 66.79 s, avg loss 0.015933, throughput 25.33K wps\n",
      "[Epoch 3 Batch 2000/5668] elapsed 65.58 s, avg loss 0.016143, throughput 25.94K wps\n",
      "[Epoch 3 Batch 3000/5668] elapsed 66.06 s, avg loss 0.015464, throughput 26.17K wps\n",
      "[Epoch 3 Batch 4000/5668] elapsed 65.47 s, avg loss 0.016213, throughput 25.97K wps\n",
      "[Epoch 3 Batch 5000/5668] elapsed 65.37 s, avg loss 0.016579, throughput 25.92K wps\n",
      "Begin Testing...\n",
      "[Batch 1000/2494] elapsed 69.96 s\n",
      "[Batch 2000/2494] elapsed 70.02 s\n",
      "[Epoch 3] train avg loss 0.016074, test acc 0.97228, test avg loss 0.013556, test auc 0.93779\n",
      "[Epoch 4 Batch 1000/5668] elapsed 67.12 s, avg loss 0.015882, throughput 25.65K wps\n",
      "[Epoch 4 Batch 2000/5668] elapsed 64.04 s, avg loss 0.016668, throughput 24.99K wps\n",
      "[Epoch 4 Batch 3000/5668] elapsed 66.33 s, avg loss 0.016121, throughput 26.25K wps\n",
      "[Epoch 4 Batch 4000/5668] elapsed 65.91 s, avg loss 0.015944, throughput 26.16K wps\n",
      "[Epoch 4 Batch 5000/5668] elapsed 65.18 s, avg loss 0.016202, throughput 25.74K wps\n",
      "Begin Testing...\n",
      "[Batch 1000/2494] elapsed 69.94 s\n",
      "[Batch 2000/2494] elapsed 70.16 s\n",
      "[Epoch 4] train avg loss 0.016076, test acc 0.97210, test avg loss 0.013998, test auc 0.93802\n"
     ]
    }
   ],
   "source": [
    "train(net, context, epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
