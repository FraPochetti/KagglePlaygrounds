{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 8, 6\n",
    "\n",
    "import spacy\n",
    "import re, string\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity='all'\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.svm import LinearSVC, SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/toxic/train.csv')#'C:\\\\Users\\\\pochetti\\\\WorkDocs\\\\Desktop\\\\Fra\\\\Francesco\\\\Kaggle\\\\toxic\\\\train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  other  \n",
       "0             0        0       0       0              0      1  \n",
       "1             0        0       0       0              0      1  \n",
       "2             0        0       0       0              0      1  \n",
       "3             0        0       0       0              0      1  \n",
       "4             0        0       0       0              0      1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_dummies = ['id', 'comment_text']\n",
    "other = df.loc[:,[col for col in df.columns if col not in not_dummies]].sum(axis=1)\n",
    "df['other'] = np.where(other == 0, 1, 0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic             15294\n",
       "severe_toxic       1595\n",
       "obscene            8449\n",
       "threat              478\n",
       "insult             7877\n",
       "identity_hate      1405\n",
       "other            143346\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:,[col for col in df.columns if col not in not_dummies]].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow0_col0 {\n",
       "            background-color:  #023858;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow0_col1 {\n",
       "            background-color:  #81aed2;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow0_col2 {\n",
       "            background-color:  #05659f;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow0_col3 {\n",
       "            background-color:  #c8cde4;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow0_col4 {\n",
       "            background-color:  #0569a4;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow0_col5 {\n",
       "            background-color:  #91b5d6;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow0_col6 {\n",
       "            background-color:  #fff7fb;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow1_col0 {\n",
       "            background-color:  #2c89bd;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow1_col1 {\n",
       "            background-color:  #023858;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow1_col2 {\n",
       "            background-color:  #2c89bd;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow1_col3 {\n",
       "            background-color:  #d1d2e6;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow1_col4 {\n",
       "            background-color:  #358fc0;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow1_col5 {\n",
       "            background-color:  #a5bddb;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow1_col6 {\n",
       "            background-color:  #b1c2de;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow2_col0 {\n",
       "            background-color:  #046198;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow2_col1 {\n",
       "            background-color:  #60a1ca;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow2_col2 {\n",
       "            background-color:  #023858;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow2_col3 {\n",
       "            background-color:  #cdd0e5;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow2_col4 {\n",
       "            background-color:  #045f95;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow2_col5 {\n",
       "            background-color:  #8bb2d4;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow2_col6 {\n",
       "            background-color:  #eae6f1;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow3_col0 {\n",
       "            background-color:  #509ac6;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow3_col1 {\n",
       "            background-color:  #b7c5df;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow3_col2 {\n",
       "            background-color:  #76aad0;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow3_col3 {\n",
       "            background-color:  #023858;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow3_col4 {\n",
       "            background-color:  #76aad0;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow3_col5 {\n",
       "            background-color:  #bcc7e1;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow3_col6 {\n",
       "            background-color:  #99b8d8;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow4_col0 {\n",
       "            background-color:  #04639b;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow4_col1 {\n",
       "            background-color:  #6ba5cd;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow4_col2 {\n",
       "            background-color:  #045e94;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow4_col3 {\n",
       "            background-color:  #cacee5;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow4_col4 {\n",
       "            background-color:  #023858;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow4_col5 {\n",
       "            background-color:  #7bacd1;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow4_col6 {\n",
       "            background-color:  #e7e3f0;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow5_col0 {\n",
       "            background-color:  #358fc0;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow5_col1 {\n",
       "            background-color:  #a2bcda;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow5_col2 {\n",
       "            background-color:  #4c99c5;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow5_col3 {\n",
       "            background-color:  #d2d3e7;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow5_col4 {\n",
       "            background-color:  #4094c3;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow5_col5 {\n",
       "            background-color:  #023858;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow5_col6 {\n",
       "            background-color:  #afc1dd;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow6_col0 {\n",
       "            background-color:  #fff7fb;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow6_col1 {\n",
       "            background-color:  #fff7fb;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow6_col2 {\n",
       "            background-color:  #fff7fb;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow6_col3 {\n",
       "            background-color:  #fff7fb;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow6_col4 {\n",
       "            background-color:  #fff7fb;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow6_col5 {\n",
       "            background-color:  #fff7fb;\n",
       "        }    #T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow6_col6 {\n",
       "            background-color:  #023858;\n",
       "        }</style>  \n",
       "<table id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0eda\" > \n",
       "<thead>    <tr> \n",
       "        <th class=\"blank level0\" ></th> \n",
       "        <th class=\"col_heading level0 col0\" >toxic</th> \n",
       "        <th class=\"col_heading level0 col1\" >severe_toxic</th> \n",
       "        <th class=\"col_heading level0 col2\" >obscene</th> \n",
       "        <th class=\"col_heading level0 col3\" >threat</th> \n",
       "        <th class=\"col_heading level0 col4\" >insult</th> \n",
       "        <th class=\"col_heading level0 col5\" >identity_hate</th> \n",
       "        <th class=\"col_heading level0 col6\" >other</th> \n",
       "    </tr></thead> \n",
       "<tbody>    <tr> \n",
       "        <th id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edalevel0_row0\" class=\"row_heading level0 row0\" >toxic</th> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow0_col0\" class=\"data row0 col0\" >1</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow0_col1\" class=\"data row0 col1\" >0.308619</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow0_col2\" class=\"data row0 col2\" >0.676515</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow0_col3\" class=\"data row0 col3\" >0.157058</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow0_col4\" class=\"data row0 col4\" >0.647518</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow0_col5\" class=\"data row0 col5\" >0.266009</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow0_col6\" class=\"data row0 col6\" >-0.967748</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edalevel0_row1\" class=\"row_heading level0 row1\" >severe_toxic</th> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow1_col0\" class=\"data row1 col0\" >0.308619</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow1_col1\" class=\"data row1 col1\" >1</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow1_col2\" class=\"data row1 col2\" >0.403014</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow1_col3\" class=\"data row1 col3\" >0.123601</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow1_col4\" class=\"data row1 col4\" >0.375807</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow1_col5\" class=\"data row1 col5\" >0.2016</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow1_col6\" class=\"data row1 col6\" >-0.298666</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edalevel0_row2\" class=\"row_heading level0 row2\" >obscene</th> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow2_col0\" class=\"data row2 col0\" >0.676515</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow2_col1\" class=\"data row2 col1\" >0.403014</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow2_col2\" class=\"data row2 col2\" >1</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow2_col3\" class=\"data row2 col3\" >0.141179</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow2_col4\" class=\"data row2 col4\" >0.741272</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow2_col5\" class=\"data row2 col5\" >0.286867</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow2_col6\" class=\"data row2 col6\" >-0.702812</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edalevel0_row3\" class=\"row_heading level0 row3\" >threat</th> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow3_col0\" class=\"data row3 col0\" >0.157058</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow3_col1\" class=\"data row3 col1\" >0.123601</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow3_col2\" class=\"data row3 col2\" >0.141179</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow3_col3\" class=\"data row3 col3\" >1</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow3_col4\" class=\"data row3 col4\" >0.150022</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow3_col5\" class=\"data row3 col5\" >0.115128</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow3_col6\" class=\"data row3 col6\" >-0.162925</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edalevel0_row4\" class=\"row_heading level0 row4\" >insult</th> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow4_col0\" class=\"data row4 col0\" >0.647518</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow4_col1\" class=\"data row4 col1\" >0.375807</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow4_col2\" class=\"data row4 col2\" >0.741272</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow4_col3\" class=\"data row4 col3\" >0.150022</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow4_col4\" class=\"data row4 col4\" >1</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow4_col5\" class=\"data row4 col5\" >0.337736</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow4_col6\" class=\"data row4 col6\" >-0.677324</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edalevel0_row5\" class=\"row_heading level0 row5\" >identity_hate</th> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow5_col0\" class=\"data row5 col0\" >0.266009</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow5_col1\" class=\"data row5 col1\" >0.2016</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow5_col2\" class=\"data row5 col2\" >0.286867</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow5_col3\" class=\"data row5 col3\" >0.115128</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow5_col4\" class=\"data row5 col4\" >0.337736</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow5_col5\" class=\"data row5 col5\" >1</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow5_col6\" class=\"data row5 col6\" >-0.280144</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edalevel0_row6\" class=\"row_heading level0 row6\" >other</th> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow6_col0\" class=\"data row6 col0\" >-0.967748</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow6_col1\" class=\"data row6 col1\" >-0.298666</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow6_col2\" class=\"data row6 col2\" >-0.702812</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow6_col3\" class=\"data row6 col3\" >-0.162925</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow6_col4\" class=\"data row6 col4\" >-0.677324</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow6_col5\" class=\"data row6 col5\" >-0.280144</td> \n",
       "        <td id=\"T_6cbba1fa_a9f2_11e8_9714_81fa5bdf0edarow6_col6\" class=\"data row6 col6\" >1</td> \n",
       "    </tr></tbody> \n",
       "</table> "
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fcd348778d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = df.loc[:,[col for col in df.columns if col not in not_dummies]].corr()\n",
    "corr.style.background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENT = 'comment_text'\n",
    "df[COMMENT].fillna(\"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119678,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(119678, 7)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(39893,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(39893, 7)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[COMMENT], \n",
    "                                                    df.loc[:,[col for col in df.columns if col not in not_dummies]], \n",
    "                                                    test_size=0.25, random_state=42)\n",
    "\n",
    "X_train.shape\n",
    "y_train.shape\n",
    "\n",
    "X_test.shape\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pochetti\\WorkDocs\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\pochetti\\WorkDocs\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"  Dulaim  That article needs some massive overhauling and we are very interested in your input .  Cheers mate !  ( Operibus anteire ) \"'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_tok = spacy.load('C:\\\\Users\\\\pochetti\\\\WorkDocs\\\\Anaconda3\\\\Lib\\\\site-packages\\\\en_core_web_sm\\\\en_core_web_sm-2.0.0')\n",
    "\n",
    "' '.join([sent.string.strip() for sent in spacy_tok(df.comment_text.sample().values[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(doc): return([tok.string.strip() for tok in spacy_tok(doc)])\n",
    "\n",
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X_train.shape[0]\n",
    "vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1 )\n",
    "trn_term_doc = vec.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_term_doc = vec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<119678x340518 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 13215659 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NbSvmClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0, dual=False, n_jobs=1):\n",
    "        self.C = C\n",
    "        self.dual = dual\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict(x.multiply(self._r))\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict_proba(x.multiply(self._r))\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # Check that X and y have correct shape\n",
    "        y = y.values\n",
    "        x, y = check_X_y(x, y, accept_sparse=True)\n",
    "\n",
    "        def pr(x, y_i, y):\n",
    "            p = x[y==y_i].sum(0)\n",
    "            return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "        self._r = sparse.csr_matrix(np.log(pr(x,1,y) / pr(x,0,y)))\n",
    "        x_nb = x.multiply(self._r)\n",
    "        self._clf = LogisticRegression(C=self.C, dual=self.dual, n_jobs=self.n_jobs).fit(x_nb, y)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit toxic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pochetti\\WorkDocs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1235: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit severe_toxic\n",
      "fit obscene\n",
      "fit threat\n",
      "fit insult\n",
      "fit identity_hate\n"
     ]
    }
   ],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "preds = np.zeros((len(X_test), len(label_cols)))\n",
    "\n",
    "for i, j in enumerate(label_cols):\n",
    "    print('fit', j)\n",
    "    m = NbSvmClassifier(C=4, dual=True, n_jobs=-1).fit(trn_term_doc, y_train[j])\n",
    "    preds[:,i] = m.predict_proba(test_term_doc)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9805751400352699"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test[label_cols].values, preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          n_jobs=-1)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = OneVsRestClassifier(LogisticRegression(), n_jobs=-1)\n",
    "clf.fit(trn_term_doc, y_train[label_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9771287489433148"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test[label_cols].values, clf.predict_proba(test_term_doc), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.read_csv('C:\\\\Users\\\\pochetti\\\\WorkDocs\\\\Desktop\\\\Fra\\\\Francesco\\\\Copperfield\\\\glove.6B\\\\glove.6B.300d.txt', sep=' ', \n",
    "                    header=None, quoting = 3)\n",
    "words = words.apply(pd.to_numeric, errors='ignore')\n",
    "d = {'word': words.loc[:,0].tolist(), 'embedding': words.loc[:,1:].values.tolist()}\n",
    "d50 = pd.DataFrame(data=d)\n",
    "w2v = d50.set_index('word')['embedding'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v['book'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_glove(X, dim=300): return np.mean([w2v[word] if word in w2v else np.zeros(dim) for word in tokenize(X)], axis=0)\n",
    "\n",
    "def add_gensim(X, dim=300): return np.mean([gens.wv[word] if word in gens.wv else np.zeros(dim) for word in tokenize(X)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = X_train.apply(add_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "tra = np.array(tr.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          n_jobs=-1)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glo = OneVsRestClassifier(LogisticRegression(), n_jobs=-1)\n",
    "glo.fit(tra, y_train[label_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tst = X_test.apply(add_glove)\n",
    "tsta = np.array(tst.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.942208144506225"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test[label_cols].values, glo.predict_proba(tsta), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = df.comment_text.apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = l.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97376954, 136103550)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gens = gensim.models.Word2Vec(documents, size=300, window=10, min_count=2, workers=10)\n",
    "\n",
    "gens.train(documents, total_examples=len(documents), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gens.wv['computer'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gens.save(\"word2vecgensim.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_gensim = X_train.apply(add_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tra_gensim = np.array(tr_gensim.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          n_jobs=-1)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = OneVsRestClassifier(LogisticRegression(), n_jobs=-1)\n",
    "g.fit(tra_gensim, y_train[label_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tst_gensim = X_test.apply(add_gensim)\n",
    "tsta_gensim = np.array(tst_gensim.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9554535195331965"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test[label_cols].values, g.predict_proba(tsta_gensim), average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GluonNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import nd, gluon, autograd\n",
    "\n",
    "import gluonnlp as nlp\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "mx.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "a = mx.nd.ones((2, 3), mx.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPoolingLayer(gluon.HybridBlock):\n",
    "    \"\"\"A block for mean pooling of encoder features\"\"\"\n",
    "    def __init__(self, prefix=None, params=None):\n",
    "        super(MeanPoolingLayer, self).__init__(prefix=prefix, params=params)\n",
    "\n",
    "    def hybrid_forward(self, F, data, valid_length): # pylint: disable=arguments-differ\n",
    "        \"\"\"Forward logic\"\"\"\n",
    "        # Data will have shape (T, N, C)\n",
    "        masked_encoded = F.SequenceMask(data,\n",
    "                                        sequence_length=valid_length,\n",
    "                                        use_sequence_length=True)\n",
    "        agg_state = F.broadcast_div(F.sum(masked_encoded, axis=0),\n",
    "                                    F.expand_dims(valid_length, axis=1))\n",
    "        return agg_state\n",
    "\n",
    "\n",
    "class SentimentNet(gluon.HybridBlock):\n",
    "    \"\"\"Network for sentiment analysis.\"\"\"\n",
    "    def __init__(self, dropout, prefix=None, params=None):\n",
    "        super(SentimentNet, self).__init__(prefix=prefix, params=params)\n",
    "        with self.name_scope():\n",
    "            self.embedding = None # will set with lm embedding later\n",
    "            self.encoder = None # will set with lm encoder later\n",
    "            self.agg_layer = MeanPoolingLayer()\n",
    "            self.output = gluon.nn.HybridSequential()\n",
    "            with self.output.name_scope():\n",
    "                self.output.add(gluon.nn.Dropout(dropout))\n",
    "                self.output.add(gluon.nn.Dense(len(label_cols), flatten=False))\n",
    "\n",
    "    def hybrid_forward(self, F, data, valid_length): # pylint: disable=arguments-differ\n",
    "        encoded = self.encoder(self.embedding(data))  # Shape(T, N, C)\n",
    "        agg_state = self.agg_layer(encoded, valid_length)\n",
    "        out = self.output(agg_state)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0\n",
    "language_model_name = 'standard_lstm_lm_200'\n",
    "pretrained = True\n",
    "learning_rate, batch_size = 0.005, 16\n",
    "bucket_num, bucket_ratio = 10, 0.2\n",
    "epochs = 1\n",
    "grad_clip = None\n",
    "log_interval = 100\n",
    "context = mx.cpu(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_model, vocab = nlp.model.get_model(name=language_model_name,\n",
    "                                      dataset_name='wikitext-2',\n",
    "                                      pretrained=pretrained,\n",
    "                                      ctx=context,\n",
    "                                      dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab(size=33278, unk=\"<unk>\", reserved=\"['<eos>']\")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentNet(\n",
      "  (embedding): HybridSequential(\n",
      "    (0): Embedding(33278 -> 200, float32)\n",
      "  )\n",
      "  (encoder): LSTM(200 -> 200, TNC, num_layers=2)\n",
      "  (agg_layer): MeanPoolingLayer(\n",
      "  \n",
      "  )\n",
      "  (output): HybridSequential(\n",
      "    (0): Dropout(p = 0, axes=())\n",
      "    (1): Dense(None -> 6, linear)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = SentimentNet(dropout=dropout)\n",
    "net.embedding = lm_model.embedding\n",
    "net.encoder = lm_model.encoder\n",
    "net.hybridize()\n",
    "net.output.initialize(mx.init.Xavier(), ctx=context)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nlp.data.SpacyTokenizer('en')\n",
    "length_clip = nlp.data.ClipSequence(500)\n",
    "\n",
    "def my_tokens(s): return vocab[length_clip(tokenize(s))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tokens('dhd djfj lllll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tok = X_train.apply(my_tokens)\n",
    "X_test_tok = X_test.apply(my_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = mx.gluon.data.ArrayDataset(X_train_tok.values, y_train[label_cols].values)\n",
    "train_data_lengths = X_train_tok.str.len().values\n",
    "\n",
    "test_dataset = mx.gluon.data.ArrayDataset(X_test_tok.values, y_test[label_cols].values)\n",
    "test_data_lengths = X_test_tok.str.len().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=119678, batch_num=5663\n",
      "  key=[59, 108, 157, 206, 255, 304, 353, 402, 451, 500]\n",
      "  cnt=[71466, 22700, 9856, 5131, 3245, 1900, 1277, 704, 580, 2819]\n",
      "  batch_size=[27, 16, 16, 16, 16, 16, 16, 16, 16, 16]\n"
     ]
    }
   ],
   "source": [
    "def get_dataloader():\n",
    "    # Construct the DataLoader\n",
    "    # Pad data, stack label and lengths\n",
    "    batchify_fn = nlp.data.batchify.Tuple(\n",
    "        nlp.data.batchify.Pad(axis=0, ret_length=True),\n",
    "        nlp.data.batchify.Stack(dtype='float32'))\n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(\n",
    "        train_data_lengths,\n",
    "        batch_size=batch_size,\n",
    "        num_buckets=bucket_num,\n",
    "        ratio=bucket_ratio,\n",
    "        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "    train_dataloader = gluon.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        batchify_fn=batchify_fn)\n",
    "    test_dataloader = gluon.data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        batchify_fn=batchify_fn)\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "train_dataloader, test_dataloader = get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "((data, length), label) = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, dataloader, context):\n",
    "    loss = gluon.loss.SigmoidBCELoss()\n",
    "    total_L = 0.0\n",
    "    total_sample_num = 0\n",
    "    total_correct_num = 0\n",
    "    start_log_interval_time = time.time()\n",
    "    print('Begin Testing...')\n",
    "    for i, ((data, valid_length), label) in enumerate(dataloader):\n",
    "        data = mx.nd.transpose(data.as_in_context(context))\n",
    "        valid_length = valid_length.as_in_context(context).astype(np.float32)\n",
    "        label = label.as_in_context(context)\n",
    "        output = net(data, valid_length)\n",
    "        L = loss(output, label)\n",
    "        pred = (output > 0.5).reshape(-1)\n",
    "        total_L += L.sum().asscalar()\n",
    "        total_sample_num += label.shape[0]\n",
    "        total_correct_num += (pred == label).sum().asscalar()\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            print('[Batch {}/{}] elapsed {:.2f} s'.format(\n",
    "                i + 1, len(dataloader),\n",
    "                time.time() - start_log_interval_time))\n",
    "            start_log_interval_time = time.time()\n",
    "    avg_L = total_L / float(total_sample_num)\n",
    "    acc = total_correct_num / float(total_sample_num)\n",
    "    return avg_L, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(data.as_in_context(context).T, length.as_in_context(context).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[ -5.16482353 -10.28565216  -5.75458097  -9.00034142  -6.30177069\n",
       "  -8.05350685]\n",
       "<NDArray 6 @cpu(0)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, context, epochs):\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'ftml',\n",
    "                            {'learning_rate': learning_rate})\n",
    "    loss = gluon.loss.SigmoidBCELoss()\n",
    "\n",
    "    parameters = net.collect_params().values()\n",
    "\n",
    "    # Training/Testing\n",
    "    for epoch in range(epochs):\n",
    "        # Epoch training stats\n",
    "        start_epoch_time = time.time()\n",
    "        epoch_L = 0.0\n",
    "        epoch_sent_num = 0\n",
    "        epoch_wc = 0\n",
    "        # Log interval training stats\n",
    "        start_log_interval_time = time.time()\n",
    "        log_interval_wc = 0\n",
    "        log_interval_sent_num = 0\n",
    "        log_interval_L = 0.0\n",
    "\n",
    "        for i, ((data, length), label) in enumerate(train_dataloader):\n",
    "            L = 0\n",
    "            wc = length.sum().asscalar()\n",
    "            log_interval_wc += wc\n",
    "            epoch_wc += wc\n",
    "            log_interval_sent_num += data.shape[1]\n",
    "            epoch_sent_num += data.shape[1]\n",
    "            with autograd.record():\n",
    "                output = net(data.as_in_context(context).T,\n",
    "                             length.as_in_context(context).astype(np.float32))\n",
    "                L = L + loss(output, label.as_in_context(context)).mean()\n",
    "            L.backward()\n",
    "            # Clip gradient\n",
    "            if grad_clip:\n",
    "                gluon.utils.clip_global_norm(\n",
    "                    [p.grad(context) for p in parameters],\n",
    "                    grad_clip)\n",
    "            # Update parameter\n",
    "            trainer.step(1)\n",
    "            log_interval_L += L.asscalar()\n",
    "            epoch_L += L.asscalar()\n",
    "            if (i + 1) % log_interval == 0:\n",
    "                print(\n",
    "                    '[Epoch {} Batch {}/{}] elapsed {:.2f} s, '\n",
    "                    'avg loss {:.6f}, throughput {:.2f}K wps'.format(\n",
    "                        epoch, i + 1, len(train_dataloader),\n",
    "                        time.time() - start_log_interval_time,\n",
    "                        log_interval_L / log_interval_sent_num, log_interval_wc\n",
    "                        / 1000 / (time.time() - start_log_interval_time)))\n",
    "                # Clear log interval training stats\n",
    "                start_log_interval_time = time.time()\n",
    "                log_interval_wc = 0\n",
    "                log_interval_sent_num = 0\n",
    "                log_interval_L = 0\n",
    "        end_epoch_time = time.time()\n",
    "        test_avg_L, test_acc = evaluate(net, test_dataloader, context)\n",
    "        print('[Epoch {}] train avg loss {:.6f}, test acc {:.2f}, '\n",
    "              'test avg loss {:.6f}, throughput {:.2f}K wps'.format(\n",
    "                  epoch, epoch_L / epoch_sent_num, test_acc, test_avg_L,\n",
    "                  epoch_wc / 1000 / (end_epoch_time - start_epoch_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 100/5663] elapsed 87.70 s, avg loss 0.001751, throughput 1.85K wps\n",
      "[Epoch 0 Batch 200/5663] elapsed 101.96 s, avg loss 0.000995, throughput 1.57K wps\n",
      "[Epoch 0 Batch 300/5663] elapsed 87.55 s, avg loss 0.000788, throughput 1.97K wps\n",
      "[Epoch 0 Batch 400/5663] elapsed 80.51 s, avg loss 0.000913, throughput 1.99K wps\n",
      "[Epoch 0 Batch 500/5663] elapsed 85.45 s, avg loss 0.000744, throughput 1.93K wps\n",
      "[Epoch 0 Batch 600/5663] elapsed 79.55 s, avg loss 0.000929, throughput 1.64K wps\n",
      "[Epoch 0 Batch 700/5663] elapsed 88.56 s, avg loss 0.000550, throughput 2.13K wps\n",
      "[Epoch 0 Batch 800/5663] elapsed 85.73 s, avg loss 0.000827, throughput 1.95K wps\n",
      "[Epoch 0 Batch 900/5663] elapsed 83.76 s, avg loss 0.000743, throughput 1.87K wps\n",
      "[Epoch 0 Batch 1000/5663] elapsed 84.24 s, avg loss 0.000660, throughput 1.86K wps\n",
      "[Epoch 0 Batch 1100/5663] elapsed 87.05 s, avg loss 0.000521, throughput 2.19K wps\n",
      "[Epoch 0 Batch 1200/5663] elapsed 93.36 s, avg loss 0.000464, throughput 2.31K wps\n",
      "[Epoch 0 Batch 1300/5663] elapsed 89.64 s, avg loss 0.000601, throughput 2.05K wps\n",
      "[Epoch 0 Batch 1400/5663] elapsed 80.62 s, avg loss 0.000624, throughput 1.94K wps\n",
      "[Epoch 0 Batch 1500/5663] elapsed 83.00 s, avg loss 0.000553, throughput 1.92K wps\n",
      "[Epoch 0 Batch 1600/5663] elapsed 89.03 s, avg loss 0.000496, throughput 2.29K wps\n",
      "[Epoch 0 Batch 1700/5663] elapsed 87.07 s, avg loss 0.000559, throughput 2.03K wps\n",
      "[Epoch 0 Batch 1800/5663] elapsed 82.89 s, avg loss 0.000616, throughput 1.98K wps\n",
      "[Epoch 0 Batch 1900/5663] elapsed 86.19 s, avg loss 0.000552, throughput 2.13K wps\n",
      "[Epoch 0 Batch 2000/5663] elapsed 84.32 s, avg loss 0.000546, throughput 2.01K wps\n",
      "[Epoch 0 Batch 2100/5663] elapsed 82.34 s, avg loss 0.000723, throughput 1.93K wps\n",
      "[Epoch 0 Batch 2200/5663] elapsed 84.95 s, avg loss 0.000641, throughput 1.97K wps\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-4da9f2f81ea5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-7ab4e89290fe>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, context, epochs)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mlog_interval_L\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mwc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fra/lib/python3.6/site-packages/mxnet/gluon/data/dataloader.py\u001b[0m in \u001b[0;36msame_process_iter\u001b[0;34m()\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0msame_process_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_sampler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m                     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batchify_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_as_in_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_pinned\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fra/lib/python3.6/site-packages/gluonnlp/data/batchify/batchify.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mele_fn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mele_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mele\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mele\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fra/lib/python3.6/site-packages/gluonnlp/data/batchify/batchify.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    254\u001b[0m             padded_arr, original_length = _pad_arrs_to_max_length(data, self._axis,\n\u001b[1;32m    255\u001b[0m                                                                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                                                                   self._dtype)\n\u001b[0m\u001b[1;32m    257\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ret_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mpadded_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fra/lib/python3.6/site-packages/gluonnlp/data/batchify/batchify.py\u001b[0m in \u001b[0;36m_pad_arrs_to_max_length\u001b[0;34m(arrs, pad_axis, pad_val, use_shared_mem, dtype)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu_shared'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_shared_mem\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0moriginal_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fra/lib/python3.6/site-packages/mxnet/ndarray/utils.py\u001b[0m in \u001b[0;36marray\u001b[0;34m(source_array, ctx, dtype)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sparse_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fra/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36marray\u001b[0;34m(source_array, ctx, dtype)\u001b[0m\n\u001b[1;32m   2433\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'source_array must be array like object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2434\u001b[0m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2435\u001b[0;31m     \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2436\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fra/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0mindexing_dispatch_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_indexing_dispatch_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexing_dispatch_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NDARRAY_BASIC_INDEXING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_nd_basic_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mindexing_dispatch_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NDARRAY_ADVANCED_INDEXING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_nd_advanced_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fra/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36m_set_nd_basic_indexing\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    708\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m                         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sync_copyfrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# value might be a list or a tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m                     \u001b[0mvalue_nd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_value_nd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fra/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36m_sync_copyfrom\u001b[0;34m(self, source_array)\u001b[0m\n\u001b[1;32m    874\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0msource_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m             ctypes.c_size_t(source_array.size)))\n\u001b[0m\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(net, context, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
