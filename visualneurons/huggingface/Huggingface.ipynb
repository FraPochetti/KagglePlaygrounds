{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T15:59:55.316372Z",
     "start_time": "2019-11-20T15:59:45.017539Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "from tqdm import trange\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from transformers import GPT2Config, OpenAIGPTConfig, XLNetConfig, TransfoXLConfig, XLMConfig, CTRLConfig\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import OpenAIGPTLMHeadModel, OpenAIGPTTokenizer\n",
    "from transformers import XLNetLMHeadModel, XLNetTokenizer\n",
    "from transformers import TransfoXLLMHeadModel, TransfoXLTokenizer\n",
    "from transformers import CTRLLMHeadModel, CTRLTokenizer\n",
    "from transformers import XLMWithLMHeadModel, XLMTokenizer\n",
    "from transformers import DistilBertTokenizer, DistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T16:39:33.826103Z",
     "start_time": "2019-11-20T16:39:33.814008Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290364"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(1, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T16:03:50.689661Z",
     "start_time": "2019-11-20T16:00:34.857432Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 231508/231508 [00:00<00:00, 364284.37B/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 492/492 [00:00<00:00, 7336.48B/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 267967963/267967963 [03:07<00:00, 1430512.45B/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "tokenizer.save_pretrained('./distilBert/')\n",
    "model.save_pretrained('./distilBert/')\n",
    "\n",
    "input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)\n",
    "outputs = model(input_ids)\n",
    "last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T16:04:12.971205Z",
     "start_time": "2019-11-20T16:04:12.931159Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3897,  0.5468, -0.3846,  ..., -0.5253,  0.0137,  0.5638],\n",
       "         [ 0.4385,  0.8767, -0.2833,  ..., -0.5621,  0.0267,  0.4095],\n",
       "         [ 0.4105,  0.7941, -0.0934,  ..., -0.5104, -0.1070,  0.9439],\n",
       "         [ 0.3925,  0.8002, -0.3922,  ..., -0.6947,  0.0348,  0.6961],\n",
       "         [ 0.1508,  0.5453, -0.3423,  ..., -0.3691,  0.1747,  0.7834],\n",
       "         [ 0.3180,  0.5482, -0.3096,  ..., -0.4357,  0.1412,  0.3862]]],\n",
       "       grad_fn=<AddcmulBackward>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T17:03:37.827319Z",
     "start_time": "2019-11-20T17:03:37.814653Z"
    }
   },
   "outputs": [],
   "source": [
    "ALL_MODELS = sum((tuple(conf.pretrained_config_archive_map.keys()) for conf in (GPT2Config, OpenAIGPTConfig, XLNetConfig, TransfoXLConfig, XLMConfig, CTRLConfig)), ())\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'gpt2': (GPT2LMHeadModel, GPT2Tokenizer),\n",
    "    'ctrl': (CTRLLMHeadModel, CTRLTokenizer),\n",
    "    'openai-gpt': (OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),\n",
    "    'xlnet': (XLNetLMHeadModel, XLNetTokenizer),\n",
    "    'transfo-xl': (TransfoXLLMHeadModel, TransfoXLTokenizer),\n",
    "    'xlm': (XLMWithLMHeadModel, XLMTokenizer),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T17:04:26.793202Z",
     "start_time": "2019-11-20T17:04:26.752679Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed(args):\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (batch size x vocabulary size)\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # scatter sorted tensors to original indexing\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "\n",
    "def sample_sequence(model, length, context, num_samples=1, temperature=1, top_k=0, top_p=0.0, repetition_penalty=1.0,\n",
    "                    is_xlnet=False, is_xlm_mlm=False, xlm_mask_token=None, xlm_lang=None, device='cpu'):\n",
    "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
    "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
    "    generated = context\n",
    "    with torch.no_grad():\n",
    "        for _ in trange(length):\n",
    "\n",
    "            inputs = {'input_ids': generated}\n",
    "            if is_xlnet: \n",
    "                # XLNet is a direct (predict same token, not next token) and bi-directional model by default\n",
    "                # => need one additional dummy token in the input (will be masked), attention mask and target mapping (see model docstring)\n",
    "                input_ids = torch.cat((generated, torch.zeros((1, 1), dtype=torch.long, device=device)), dim=1)\n",
    "                perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float, device=device)\n",
    "                perm_mask[:, :, -1] = 1.0  # Previous tokens don't see last token\n",
    "                target_mapping = torch.zeros((1, 1, input_ids.shape[1]), dtype=torch.float, device=device)\n",
    "                target_mapping[0, 0, -1] = 1.0  # predict last token\n",
    "                inputs = {'input_ids': input_ids, 'perm_mask': perm_mask, 'target_mapping': target_mapping}\n",
    "\n",
    "            if is_xlm_mlm and xlm_mask_token:\n",
    "                # XLM MLM models are direct models (predict same token, not next token)\n",
    "                # => need one additional dummy token in the input (will be masked and guessed)\n",
    "                input_ids = torch.cat((generated, torch.full((1, 1), xlm_mask_token, dtype=torch.long, device=device)), dim=1)\n",
    "                inputs = {'input_ids': input_ids}\n",
    "\n",
    "            if xlm_lang is not None:\n",
    "                inputs[\"langs\"] = torch.tensor([xlm_lang] * inputs[\"input_ids\"].shape[1], device=device).view(1, -1)\n",
    "\n",
    "            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet/CTRL (cached hidden-states)\n",
    "            next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)\n",
    "\n",
    "            # repetition penalty from CTRL (https://arxiv.org/abs/1909.05858)\n",
    "            for i in range(num_samples):\n",
    "                for _ in set(generated[i].tolist()):\n",
    "                    next_token_logits[i, _] /= repetition_penalty\n",
    "                \n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "            if temperature == 0: # greedy sampling:\n",
    "                next_token = torch.argmax(filtered_logits, dim=-1).unsqueeze(-1)\n",
    "            else:\n",
    "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T07:46:33.447784Z",
     "start_time": "2019-11-19T07:46:33.440805Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt2',\n",
       " 'gpt2-medium',\n",
       " 'gpt2-large',\n",
       " 'distilgpt2',\n",
       " 'openai-gpt',\n",
       " 'xlnet-base-cased',\n",
       " 'xlnet-large-cased',\n",
       " 'transfo-xl-wt103',\n",
       " 'xlm-mlm-en-2048',\n",
       " 'xlm-mlm-ende-1024',\n",
       " 'xlm-mlm-enfr-1024',\n",
       " 'xlm-mlm-enro-1024',\n",
       " 'xlm-mlm-tlm-xnli15-1024',\n",
       " 'xlm-mlm-xnli15-1024',\n",
       " 'xlm-clm-enfr-1024',\n",
       " 'xlm-clm-ende-1024',\n",
       " 'xlm-mlm-17-1280',\n",
       " 'xlm-mlm-100-1280',\n",
       " 'ctrl')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T07:42:10.563271Z",
     "start_time": "2019-11-18T07:42:10.559281Z"
    }
   },
   "outputs": [],
   "source": [
    "#?? tokenizer_class.from_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T17:04:14.582265Z",
     "start_time": "2019-11-20T17:04:07.295700Z"
    }
   },
   "outputs": [],
   "source": [
    "device=torch.device(\"cpu\")#torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_class, tokenizer_class = MODEL_CLASSES['gpt2']\n",
    "tokenizer = tokenizer_class.from_pretrained('./distilGPT2/')\n",
    "model = model_class.from_pretrained('./distilGPT2/')\n",
    "model.to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T07:42:13.968193Z",
     "start_time": "2019-11-18T07:42:13.675943Z"
    }
   },
   "outputs": [],
   "source": [
    "#tokenizer.save_pretrained('.')\n",
    "model.save_pretrained('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T17:04:16.638091Z",
     "start_time": "2019-11-20T17:04:16.630183Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_text = \"Hi, how are you\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T17:04:34.452038Z",
     "start_time": "2019-11-20T17:04:34.443591Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17250, 11, 703, 389, 345]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_tokens = tokenizer.encode(raw_text, add_special_tokens=False)\n",
    "context_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T07:42:13.988110Z",
     "start_time": "2019-11-18T07:42:13.983124Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(context_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T17:06:04.359221Z",
     "start_time": "2019-11-20T17:06:04.028902Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  7.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 7])\n",
      " going to\n",
      " working?\n",
      " guys getting\n"
     ]
    }
   ],
   "source": [
    "out = sample_sequence(\n",
    "            model=model,\n",
    "            context=context_tokens,\n",
    "            num_samples=3,\n",
    "            length=2,\n",
    "            temperature=1,\n",
    "            top_k=0,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.0,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "print(out.shape)\n",
    "\n",
    "out = out[:, len(context_tokens):].tolist()\n",
    "d = {}\n",
    "for i, o in enumerate(out):\n",
    "    text = tokenizer.decode(o, clean_up_tokenization_spaces=True)\n",
    "    text = text[:None]\n",
    "    d[f'text{i}'] = text\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T17:07:13.720595Z",
     "start_time": "2019-11-20T17:07:13.712848Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"text0\": \" going to\", \"text1\": \" working?\", \"text2\": \" guys getting\"}'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json.dumps(d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai]",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
